# 小红书自动化内容生成与发布系统 - 实现文档

## 1. 项目简介
本项目是一个全自动化的 Python 工具集，旨在从互联网采集特定主题（网购/拆快递/退货/漫画）的内容，经过清洗和标准化处理后，自动发布到小红书平台。

主要包含三个核心模块：
1. **数据采集 (crawler.py)**: 基于 Selenium 自动化搜索关键词，**进入详情页**爬取标题、正文及**所有图片**。
2. **数据处理 (processor.py)**: 清洗数据，过滤低质量图片，生成标准化的发布文案（支持多图）。
3. **自动发布 (publisher.py)**: 模拟人工操作，自动上传**多张图片**并填写标题正文发布。

## 2. 环境与依赖
- **操作系统**: Windows (推荐) / macOS / Linux
- **Python 版本**: 3.8+
- **浏览器**: Microsoft Edge (必需，因使用了 Edge 驱动)
- **核心依赖库**:
  - `selenium`: 网页自动化控制
  - `webdriver_manager`: 自动管理浏览器驱动
  - `requests`: 图片下载
  - `Pillow` (PIL): 图片格式验证与处理

## 3. 文件结构说明
```text
project_root/
├── crawler.py          # [入口1] 爬虫脚本 (更新：支持详情页多图抓取)
├── processor.py        # [入口2] 数据处理脚本 (更新：适配多图结构)
├── publisher.py        # [入口3] 自动发布脚本 (更新：支持多图上传)
├── get_cookies.py      # [工具] 手动提取 Cookie 脚本
├── cookies.json        # 登录凭证 (自动生成)
├── msedgedriver.exe    # 浏览器驱动 (自动下载或手动放置)
├── 实现.md             # 本文档
└── data/               # 数据存储目录
    ├── images/         # 爬取的原始图片
    ├── raw_data.json   # 爬取的原始元数据 (结构已更新)
    └── result.json     # 处理后待发布的数据
```

## 4. 详细实现流程

### 第一步：数据采集 (crawler.py)
**功能**: 根据关键词搜索笔记，进入详情页抓取完整信息。

**核心逻辑**:
1. **驱动初始化**:
   - 自动下载匹配版本的 Edge 驱动。
   - 配置 `excludeSwitches` 和 `useAutomationExtension` 隐藏自动化特征。
   - 屏蔽控制台常规日志 (`--log-level=3`)。
2. **登录管理**:
   - 优先加载本地 `cookies.json`。
   - 具备**强校验登录检测**功能：必须包含 `web_session` 字段，且验证页面元素。
3. **内容抓取 (重大更新)**:
   - 搜索关键词：`"网购 拆快递 退货 六宫格漫画"`。
   - **列表页解析**: 提取笔记详情页链接 (`/explore/xxxx`)。
   - **详情页抓取**:
     - 自动打开新标签页进入详情。
     - 提取 **标题** 和 **正文描述**。
     - 解析图片轮播组件 (`.swiper-slide`)，提取 **所有高清图片**。
     - 下载图片到 `data/images`，并生成包含图片列表的元数据。

### 第二步：数据清洗 (processor.py)
**功能**: 对采集的数据进行质量控制和格式化。

**核心逻辑**:
1. **多图验证**:
   - 遍历每篇笔记的所有图片。
   - 验证文件存在性及分辨率（>300x300）。
   - 剔除无效图片，若有效图片数量为0则丢弃整条数据。
2. **文本处理**:
   - 组合 `标题` + `正文` + `Tag` 生成最终发布文案。
   - 自动添加热门标签：`#网购 #搞笑 #日常 #避坑指南 #拆快递 #退货 #漫画分享 #六宫格`。
3. **输出生成**:
   - 生成的新数据结构包含 `images` (列表) 和 `title` (独立字段)。

### 第三步：自动发布 (publisher.py)
**功能**: 将处理好的内容自动发布到小红书创作者中心。

**核心逻辑**:
1. **环境复用**: 使用与爬虫相同的浏览器配置和 Cookie 文件。
2. **多图发布流程**:
   - 自动打开发布页。
   - 模拟文件上传：将**多张图片路径**拼接后一次性上传。
   - 自动填写标题（截取自原文）和正文。
3. **安全机制**:
   - **随机延时**: 模拟真人操作间隔。
   - **防封号限制**: 默认每次运行最多发布 3 条内容。
   - **模拟模式**: 默认注释了“点击发布”动作，仅演示流程。

## 5. 关键技术点与反爬策略

1. **Cookie 深度管理**:
   - 增加了对 `web_session` 核心字段的强制检查，防止无效 Cookie 导致程序空转。
   - 优化了 Cookie 注入逻辑，确保 `domain` 属性正确。

2. **隐匿性增强**:
   - 屏蔽了 `enable-logging` 等 Selenium 默认日志，减少控制台干扰。
   - 使用 CDP 命令修改 `navigator.webdriver` 属性。

3. **详情页抓取策略**:
   - 使用 `window.open` 打开新标签页，避免破坏列表页的滚动状态。
   - 使用 `driver.switch_to.window` 灵活切换标签页。
   - 兼容多种 DOM 结构（如 `swiper` 轮播图或单图结构）的解析。

4. **混合点击机制 (新增)**:
   - 针对部分高反爬场景下的“元素不可交互”问题，实现了三级点击策略：
     1. **常规 Selenium 点击**: 标准 API，速度快。
     2. **PyAutoGUI 物理点击**: 模拟真实鼠标移动和点击，绕过透明遮罩或事件拦截。
     3. **JS 强制点击**: 直接触发 DOM 事件作为最后兜底。

## 6. 使用说明
1. 运行 `python crawler.py`，首次需扫码登录。
   - **注意**: 程序在点击笔记时可能会移动鼠标（PyAutoGUI 模式），运行期间请勿操作鼠标。
2. 运行 `python processor.py`，清洗数据。
3. 运行 `python publisher.py`，观察浏览器自动上传多张图片并填写内容。
