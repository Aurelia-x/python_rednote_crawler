# -*- coding: utf-8 -*-
# MERGED FILE: DO NOT EDIT MANUALLY
# This file is auto-generated by merging the entire execution flow for a specific command.

import asyncio
import json
import os
import sys
import re
import time
import random
import functools
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Tuple, Union
from urllib.parse import urlparse

import aiofiles
import httpx
from playwright.async_api import Browser, BrowserContext, Page, async_playwright
from tenacity import RetryError, retry, retry_if_result, stop_after_attempt, wait_fixed
from rich.console import Console
from rich.logging import RichHandler
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn
from rich.table import Table
import logging
from concurrent.futures import ThreadPoolExecutor
from PIL import Image
import qrcode
import io
import hashlib
import ctypes
import uuid
from urllib.parse import quote as url_quote

# -----------------------------------------------------------------------------
# Sign-related functions (from xhs_sign.py and playwright_sign.py)
# -----------------------------------------------------------------------------

BASE64_CHARS = list("ZmserbBoHQtNP+wOcza/LpngG8yJq42KWYj0DSfdikx3VT16IlUAFM97hECvuRX5")
CRC32_TABLE = [
    0, 1996959894, 3993919788, 2567524794, 124634137, 1886057615, 3915621685,
    2657392035, 249268274, 2044508324, 3772115230, 2547177864, 162941995,
    2125561021, 3887607047, 2428444049, 498536548, 1789927666, 4089016648,
    2227061214, 450548861, 1843258603, 4107580753, 2211677639, 325883990,
    1684777152, 4251122042, 2321926636, 335633487, 1661365465, 4195302755,
    2366115317, 997073096, 1281953886, 3579855332, 2724688242, 1006888145,
    1258607687, 3524101629, 2768942443, 901097722, 1119000684, 3686517206,
    2898065728, 853044451, 1172266101, 3705015759, 2882616665, 651767980,
    1373503546, 3369554304, 3218104598, 565507253, 1454621731, 3485111705,
    3099436303, 671266974, 1594198024, 3322730930, 2970347812, 795835527,
    1483230225, 3244367275, 3060149565, 1994146192, 31158534, 2563907772,
    4023717930, 1907459465, 112637215, 2680153253, 3904427059, 2013776290,
    251722036, 2517215374, 3775830040, 2137656763, 141376813, 2439277719,
    3865271297, 1802195444, 476864866, 2238001368, 4066508878, 1812370925,
    453092731, 2181625025, 4111451223, 1706088902, 314042704, 2344532202,
    4240017532, 1658658271, 366619977, 2362670323, 4224994405, 1303535960,
    984961486, 2747007092, 3569037538, 1256170817, 1037604311, 2765210733,
    3554079995, 1131014506, 879679996, 2909243462, 3663771856, 1141124467,
    855842277, 2852801631, 3708648649, 1342533948, 654459306, 3188396048,
    3373015174, 1466479909, 544179635, 3110523913, 3462522015, 1591671054,
    702138776, 2966460450, 3352799412, 1504918807, 783551873, 3082640443,
    3233442989, 3988292384, 2596254646, 62317068, 1957810842, 3939845945,
    2647816111, 81470997, 1943803523, 3814918930, 2489596804, 225274430,
    2053790376, 3826175755, 2466906013, 167816743, 2097651377, 4027552580,
    2265490386, 503444072, 1762050814, 4150417245, 2154129355, 426522225,
    1852507879, 4275313526, 2312317920, 282753626, 1742555852, 4189708143,
    2394877945, 397917763, 1622183637, 3604390888, 2714866558, 953729732,
    1340076626, 3518719985, 2797360999, 1068828381, 1219638859, 3624741850,
    2936675148, 906185462, 1090812512, 3747672003, 2825379669, 829329135,
    1181335161, 3412177804, 3160834842, 628085408, 1382605366, 3423369109,
    3138078467, 570562233, 1426400815, 3317316542, 2998733608, 733239954,
    1555261956, 3268935591, 3050360625, 752459403, 1541320221, 2607071920,
    3965973030, 1969922972, 40735498, 2617837225, 3943577151, 1913087877,
    83908371, 2512341634, 3803740692, 2075208622, 213261112, 2463272603,
    3855990285, 2094854071, 198958881, 2262029012, 4057260610, 1759359992,
    534414190, 2176718541, 4139329115, 1873836001, 414664567, 2282248934,
    4279200368, 1711684554, 285281116, 2405801727, 4167216745, 1634467795,
    376229701, 2685067896, 3608007406, 1308918612, 956543938, 2808555105,
    3495958263, 1231636301, 1047427035, 2932959818, 3654703836, 1088359270,
    936918000, 2847714899, 3736837829, 1202900863, 817233897, 3183342108,
    3401237130, 1404277552, 615818150, 3134207493, 3453421203, 1423857449,
    601450431, 3009837614, 3294710456, 1567103746, 711928724, 3020668471,
    3272380065, 1510334235, 755167117,
]

def _right_shift_unsigned(num: int, bit: int = 0) -> int:
    val = ctypes.c_uint32(num).value >> bit
    MAX32INT = 4294967295
    return (val + (MAX32INT + 1)) % (2 * (MAX32INT + 1)) - MAX32INT - 1

def mrc(e: str) -> int:
    o = -1
    for n in range(min(57, len(e))):
        o = CRC32_TABLE[(o & 255) ^ ord(e[n])] ^ _right_shift_unsigned(o, 8)
    return o ^ -1 ^ 3988292384

def _triplet_to_base64(e: int) -> str:
    return (
        BASE64_CHARS[(e >> 18) & 63]
        + BASE64_CHARS[(e >> 12) & 63]
        + BASE64_CHARS[(e >> 6) & 63]
        + BASE64_CHARS[e & 63]
    )

def _encode_chunk(data: list, start: int, end: int) -> str:
    result = []
    for i in range(start, end, 3):
        c = ((data[i] << 16) & 0xFF0000) + ((data[i + 1] << 8) & 0xFF00) + (data[i + 2] & 0xFF)
        result.append(_triplet_to_base64(c))
    return "".join(result)

def encode_utf8(s: str) -> list:
    encoded = url_quote(s, safe="~()*!.'")
    result = []
    i = 0
    while i < len(encoded):
        if encoded[i] == "%" :
            result.append(int(encoded[i + 1: i + 3], 16))
            i += 3
        else:
            result.append(ord(encoded[i]))
            i += 1
    return result

def b64_encode(data: list) -> str:
    length = len(data)
    remainder = length % 3
    chunks = []
    main_length = length - remainder
    for i in range(0, main_length, 16383):
        chunks.append(_encode_chunk(data, i, min(i + 16383, main_length)))
    if remainder == 1:
        a = data[length - 1]
        chunks.append(BASE64_CHARS[a >> 2] + BASE64_CHARS[(a << 4) & 63] + "==")
    elif remainder == 2:
        a = (data[length - 2] << 8) + data[length - 1]
        chunks.append(
            BASE64_CHARS[a >> 10] + BASE64_CHARS[(a >> 4) & 63] + BASE64_CHARS[(a << 2) & 63] + "="
        )
    return "".join(chunks)

def get_trace_id() -> str:
    return "".join(random.choice("abcdef0123456789") for _ in range(16))

def _build_sign_string(uri: str, data: Optional[Union[Dict, str]] = None, method: str = "POST") -> str:
    if method.upper() == "POST":
        c = uri
        if data is not None:
            if isinstance(data, dict):
                c += json.dumps(data, separators=(",", ":"), ensure_ascii=False)
            elif isinstance(data, str):
                c += data
        return c
    else:
        if not data or (isinstance(data, dict) and len(data) == 0):
            return uri
        if isinstance(data, dict):
            params = []
            for key, value in data.items():
                value_str = str(value)
                value_str = url_quote(value_str, safe='')
                params.append(f"{key}={value_str}")
            return f"{uri}?{'&'.join(params)}"
        elif isinstance(data, str):
            return f"{uri}?{data}"
        return uri

def _md5_hex(s: str) -> str:
    return hashlib.md5(s.encode("utf-8")).hexdigest()

def _build_xs_payload(x3_value: str, data_type: str = "object") -> str:
    s = {
        "x0": "4.2.1",
        "x1": "xhs-pc-web",
        "x2": "Mac OS",
        "x3": x3_value,
        "x4": data_type,
    }
    return "XYS_" + b64_encode(encode_utf8(json.dumps(s, separators=(",", ":"))))

def _build_xs_common(a1: str, b1: str, x_s: str, x_t: str) -> str:
    payload = {
        "s0": 3, "s1": "", "x0": "1", "x1": "4.2.2", "x2": "Mac OS",
        "x3": "xhs-pc-web", "x4": "4.74.0", "x5": a1, "x6": x_t,
        "x7": x_s, "x8": b1, "x9": mrc(x_t + x_s + b1), "x10": 154,
        "x11": "normal",
    }
    return b64_encode(encode_utf8(json.dumps(payload, separators=(",", ":"))))

async def get_b1_from_localstorage(page: Page) -> str:
    try:
        local_storage = await page.evaluate("() => window.localStorage")
        return local_storage.get("b1", "")
    except Exception:
        return ""

async def call_mnsv2(page: Page, sign_str: str, md5_str: str) -> str:
    sign_str_escaped = sign_str.replace("\\", "\\\\").replace("'", "\\'").replace("\n", "\\n")
    md5_str_escaped = md5_str.replace("\\", "\\\\").replace("'", "\\'")
    try:
        result = await page.evaluate(f"window.mnsv2('{sign_str_escaped}', '{md5_str_escaped}')")
        return result if result else ""
    except Exception:
        return ""


# -----------------------------------------------------------------------------
# Config - Hardcoded settings from the command:
# python main.py --platform xhs --lt cookie --type search --save_data_option json
# -----------------------------------------------------------------------------

def get_search_id():
    """
    Generates a unique search ID for tracking search sessions.
    This is a simplified version of the original implementation.
    """
    return str(uuid.uuid4())

class Config:
    """
    Configuration class for the crawler, replacing arg-parsing for the merged script.
    """
    # General settings
    PLATFORM = "xhs"
    LOGIN_TYPE = "cookie"
    CRAWLER_TYPE = "search"
    SAVE_DATA_OPTION = "json"

    # File paths
    COOKIE_FILE_PATH = "cookies.json"  # Path to the cookie file

    # User-configurable values
    KEYWORDS = "python,java" # Example keywords, comma-separated
    
    # Default settings from original config.py
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
    TIMEOUT = 10
    CRAWLER_MAX_NOTES_COUNT = 20
    SEARCH_PAGE_SCROLL_TIMES = 1
    COMMENT_PAGE_SCROLL_TIMES = 1
    HTTP_PROXY = "" # "http://127.0.0.1:7890"
    
    # Playwright settings
    HEADLESS = True
    
    # Logging
    LOG_LEVEL = "INFO"
    
# Initialize config object
config = Config()

# -----------------------------------------------------------------------------
# Abstract Base Classes
# -----------------------------------------------------------------------------

class AbstractLogin(ABC):
    @abstractmethod
    async def login(self):
        pass

class AbstractCrawler(ABC):
    @abstractmethod
    async def start(self):
        pass

    @abstractmethod
    async def search(self):
        pass

class AbstractApiClient(ABC):
    @abstractmethod
    async def request(self, method, url, **kwargs):
        pass

# -----------------------------------------------------------------------------
# Merged Code from /tools/
# -----------------------------------------------------------------------------

# from tools.time_util
def get_current_timestamp() -> int:
    return int(time.time() * 1000)

def get_current_time() -> str:
    return time.strftime('%Y-%m-%d %X', time.localtime())

def get_current_date() -> str:
    return time.strftime('%Y-%m-%d', time.localtime())

# from tools.crawler_util
def get_user_agent() -> str:
    ua_list = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
    ]
    return random.choice(ua_list)

def load_cookies():
    """
    加载本地保存的 Cookie
    """
    if os.path.exists(COOKIES_PATH):
        try:
            with open(COOKIES_PATH, "r", encoding="utf-8") as f:
                content = f.read()
                if not content:
                    return False
                cookie_list = json.loads(content)
                
            if not cookie_list:
                print("Cookie 列表为空，视为无效")
                return False
                
            has_session = any(c.get('name') == 'web_session' for c in cookie_list)
            if not has_session:
                print("Cookie 中缺失 web_session 核心凭证，视为无效")
                return False
                
            driver.get("https://www.xiaohongshu.com")
            time.sleep(2)
            
            for ck in cookie_list:
                ck.pop("sameSite", None)
                ck.pop("storeId", None)
                if ck.get("domain", "").startswith("."):
                    ck["domain"] = ck["domain"][1:]
                try:
                    driver.add_cookie(ck)
                except:
                    pass
            
            driver.refresh()
            time.sleep(2)
            return True
        except Exception as e:
            print(f"加载 Cookie 失败: {e}")
            return False
    return False


def convert_cookies(cookies: Optional[List[Dict]]) -> Tuple[str, Dict]:
    if not cookies:
        return "", {}
    cookies_str = ";".join([f"{cookie.get('name')}={cookie.get('value')}" for cookie in cookies])
    cookie_dict = {cookie.get('name'): cookie.get('value') for cookie in cookies}
    return cookies_str, cookie_dict

def convert_str_cookie_to_dict(cookie_str: str) -> Dict:
    cookie_dict: Dict[str, str] = {}
    if not cookie_str:
        return cookie_dict
    for cookie in cookie_str.split(";"):
        cookie = cookie.strip()
        if not cookie:
            continue
        parts = cookie.split("=", 1)
        if len(parts) == 2:
            cookie_dict[parts[0]] = parts[1]
    return cookie_dict

def extract_text_from_html(html: str) -> str:
    if not html:
        return ""
    clean_html = re.sub(r'<(script|style)[^>]*>.*?</\1>', '', html, flags=re.DOTALL)
    return re.sub(r'<[^>]+>', '', clean_html).strip()

# from tools.async_file_writer
class AsyncFileWriter:
    """
    Asynchronously writes data to a file, ensuring thread safety with a lock.
    """
    # ... (existing AsyncFileWriter code) ...

# ... (previous code in merge.py) ...

class XiaoHongShuLogin(AbstractLogin):
    """
    Handles the login process for XiaoHongShu, supporting both QR code and cookie-based login.
    """
    def __init__(self, login_type: str, browser: "Browser", context: "BrowserContext", cookie_list: List[Dict]):
        self.login_type = login_type
        self.browser = browser
        self.browser_context = context
        self.cookie_list = cookie_list

    async def login(self):
        if self.login_type == "cookie":
            await self.login_by_cookies()
        elif self.login_type == "qr":
            await self.login_by_qrcode()
        else:
            raise ValueError("Invalid Login Type. Only 'qr' or 'cookie' are supported.")

    async def login_by_qrcode(self):
        logger.info("QR code login is not implemented in this merged script.")
        pass

    async def login_by_cookies(self):
        logger.info("Attempting to log in with cookies from file...")
        
        # Add cookies to the browser context
        await self.browser_context.add_cookies(self.cookie_list)
        logger.info(f"Successfully added {len(self.cookie_list)} cookies to the browser context.")
        
        # Verify login status by visiting a protected page
        page = await self.browser_context.new_page()
        await page.goto("https://www.xiaohongshu.com/explore")
        await asyncio.sleep(2)  # Wait for potential redirection
        
        if "login" in page.url:
            logger.error("Login failed. The browser was redirected to the login page. Please check if your cookies are valid or expired.")
        else:
            logger.info("Login successful! The browser is on a protected page.")
            
        await page.close()

# ... (previous code in merge.py) ...

class XiaoHongShuClient(AbstractApiClient):
    def __init__(
        self,
        timeout=60,
        proxy=None,
        *,
        headers: Dict[str, str],
        playwright_page: "Page",
        cookie_dict: Dict[str, str],
    ):
        self.proxy = proxy
        self.timeout = timeout
        self.headers = headers
        self._host = "https://edith.xiaohongshu.com"
        self.playwright_page = playwright_page
        self.cookie_dict = cookie_dict

class XiaoHongShuCrawler(AbstractCrawler):
    def __init__(self, crawler_type: str, config: Config):
        self.crawler_type = crawler_type
        self.config = config
        self.index_url = "https://www.xiaohongshu.com"
        self.user_agent = self.config.USER_AGENT
        self.file_writer = AsyncFileWriter(self.config.PLATFORM, crawler_type)
        self.page = None
        self.client = None

    async def start(self):
        """
        Main entry point for the crawler.
        Initializes Playwright, loads cookies, logs in, and starts the search process.
        """
        # Load cookies from file
        cookie_list, cookie_dict = self.load_cookies_from_file()
        if not cookie_list:
            return

        async with async_playwright() as playwright:
            browser = await playwright.chromium.launch(headless=self.config.HEADLESS)
            context = await browser.new_context(user_agent=self.user_agent)
            self.page = await context.new_page()
            await self.page.goto(self.index_url)

            # Perform login using the loaded cookies
            login = XiaoHongShuLogin(
                login_type=self.config.LOGIN_TYPE,
                browser=browser,
                context=context,
                cookie_list=cookie_list
            )
            await login.login()

            # Initialize the API client with the loaded cookies
            self.client = self.init_client(cookie_dict)
            await self.client.update_cookies(context)

            # Start the search process
            await self.search()

            await browser.close()

    def load_cookies_from_file(self) -> Tuple[Optional[List[Dict]], Optional[Dict]]:
        """
        Loads cookies from the specified JSON file.
        Returns a tuple of (cookie_list, cookie_dict).
        """
        cookie_path = self.config.COOKIE_FILE_PATH
        if not os.path.exists(cookie_path):
            logger.error(f"Cookie file not found at: {cookie_path}")
            return None, None
        
        try:
            with open(cookie_path, "r", encoding="utf-8") as f:
                cookie_list = json.load(f)
            
            if not isinstance(cookie_list, list):
                logger.error("Cookie file content is not a valid JSON list.")
                return None, None

            # Convert list of cookies to a dictionary for the client
            cookie_dict = {cookie['name']: cookie['value'] for cookie in cookie_list}
            
            # Validate essential cookies
            if "web_session" not in cookie_dict:
                logger.error("Essential cookie 'web_session' not found in cookie file.")
                return None, None
                
            logger.info(f"Successfully loaded {len(cookie_list)} cookies from {cookie_path}")
            return cookie_list, cookie_dict
        except Exception as e:
            logger.error(f"Failed to load or parse cookie file: {e}")
            return None, None

    def init_client(self, cookie_dict: Dict) -> "XiaoHongShuClient":
        """Initialize xhs client"""
        return XiaoHongShuClient(
            headers={"User-Agent": self.user_agent},
            playwright_page=self.page,
            cookie_dict=cookie_dict
        )

    async def search(self):
        logger.info("Starting search...")
        for keyword in self.config.KEYWORDS.split(","):
            logger.info(f"Searching for keyword: {keyword}")
            page_num = 1
            while True:
                try:
                    res = await self.client.get_note_by_keyword(keyword, page=page_num)
                    if not res.get("items"):
                        logger.info("No more items found.")
                        break
                    
                    for item in res["items"]:
                        if item.get("model_type") == "note":
                            await self.file_writer.write_single_item_to_json(item, "notes")
                            logger.info(f"Saved note: {item.get('id')}")

                    if not res.get("has_more"):
                        logger.info("No more pages.")
                        break
                    
                    page_num += 1
                    await asyncio.sleep(random.uniform(1.5, 2.5))
                except Exception as e:
                    logger.error(f"An error occurred while searching: {e}")
                    break

    async def _pre_headers(self, url: str, data: dict, method: str = "POST") -> Dict:
        signs = await self.sign(url, data, method)
        headers = {
            "X-S": signs["x-s"],
            "X-T": str(signs["x-t"]),
            "X-S-Common": signs["x-s-common"],
            "X-B3-Traceid": signs["x-b3-traceid"],
        }
        self.headers.update(headers)
        return self.headers

    async def request(self, method, url, **kwargs) -> Union[str, Any]:
        async with httpx.AsyncClient(proxies=self.proxy) as client:
            response = await client.request(method, url, timeout=self.timeout, **kwargs)
        data: Dict = response.json()
        if data["success"]:
            return data.get("data", {})
        else:
            raise Exception(data.get("msg", "Unknown error"))

    async def get(self, uri: str, params: dict) -> Dict:
        headers = await self._pre_headers(uri, params, method="GET")
        return await self.request("GET", f"{self._host}{uri}", params=params, headers=headers)

    async def post(self, uri: str, data: dict) -> Dict:
        headers = await self._pre_headers(uri, data, method="POST")
        return await self.request("POST", f"{self._host}{uri}", json=data, headers=headers)

    async def get_note_by_keyword(self, keyword: str, page: int = 1, page_size: int = 20) -> Dict:
        uri = "/api/sns/web/v1/search/notes"
        data = {
            "keyword": keyword,
            "page": page,
            "page_size": page_size,
            "search_id": get_search_id(),
            "sort": "general",
            "note_type": 0,
        }
        return await self.post(uri, data)

    async def sign(self, uri: str, data: Optional[Union[Dict, str]], method: str = "POST") -> Dict[str, Any]:
        """Generate complete signature request headers via playwright"""
        a1 = self.cookie_dict.get("a1", "")
        b1 = await get_b1_from_localstorage(self.playwright_page)
        sign_str = _build_sign_string(uri, data, method)
        md5_str = _md5_hex(sign_str)
        x_s = await call_mnsv2(self.playwright_page, sign_str, md5_str)
        x_t = str(int(time.time() * 1000))
        return {
            "x-s": x_s,
            "x-t": x_t,
            "x-s-common": _build_xs_common(a1, b1, x_s, x_t),
            "x-b3-traceid": get_trace_id(),
        }

    async def update_cookies(self, context: "BrowserContext"):
        cookie_dict = {cookie['name']: cookie['value'] for cookie in await context.cookies()}
        self.cookie_dict = cookie_dict
        self.headers['Cookie'] = "; ".join([f"{k}={v}" for k, v in cookie_dict.items()])
        logger.info("Cookies updated.")

    """
    Handles the login process for XiaoHongShu, supporting both QR code and cookie-based login.
    """
    def __init__(self, login_type: str, browser: Browser, context: BrowserContext, cookie_str: str):
        """
        Initializes the XiaoHongShuLogin instance.
        Args:
            login_type: The method to use for login ('qr', 'cookie').
            browser: The Playwright Browser instance.
            context: The Playwright BrowserContext for the current session.
            cookie_str: The cookie string to be used for login.
        """
        self.login_type = login_type
        self.browser = browser
        self.browser_context = context
        self.cookie_str = cookie_str

    async def login(self):
        """
        Dispatches the login request to the appropriate method based on the login_type.
        """
        if self.login_type == "cookie":
            await self.login_by_cookies()
        elif self.login_type == "qr":
            await self.login_by_qrcode()
        else:
            raise ValueError("Invalid Login Type Currently only supported for qrcode or cookie ...")

    async def login_by_qrcode(self):
        """
        Guides the user through a QR code-based login process.
        """
        logger.info("[XiaoHongShuLogin.login_by_qrcode] Begin login xiaohongshu by qrcode ...")
        # ... (QR code login logic not included as it's not part of the requested flow) ...
        pass

    async def login_by_cookies(self):
        """
        Logs into the XiaoHongShu website by injecting a cookie into the browser context.
        It specifically looks for and uses the 'web_session' from the provided cookie string.
        """
        logger.info("[XiaoHongShuLogin.login_by_cookies] Begin login xiaohongshu by cookie ...")
        # Convert the cookie string to a dictionary
        cookie_dict = convert_str_cookie_to_dict(self.cookie_str)
        
        # Filter for the 'web_session' cookie, as it's the primary session identifier
        for key, value in cookie_dict.items():
            if key == "web_session":
                await self.browser_context.add_cookies([{
                    'name': key,
                    'value': value,
                    'domain': ".xiaohongshu.com",
                    'path': "/"
                }])
                logger.info(f"[XiaoHongShuLogin.login_by_cookies] Successfully added 'web_session' cookie.")
                break  # Exit after finding and setting the session cookie
        else:
            logger.warning("[XiaoHongShuLogin.login_by_cookies] 'web_session' not found in cookie string.")

    def __init__(self, platform: str, crawler_type: str):
        self.lock = asyncio.Lock()
        self.platform = platform
        self.crawler_type = crawler_type

    def _get_file_path(self, file_type: str, item_type: str) -> str:
        base_path = f"data/{self.platform}/{file_type}"
        os.makedirs(base_path, exist_ok=True)
        file_name = f"{self.crawler_type}_{item_type}_{get_current_date()}.{file_type}"
        return os.path.join(base_path, file_name)

    async def write_single_item_to_json(self, item: Dict, item_type: str):
        file_path = self._get_file_path('json', item_type)
        async with self.lock:
            existing_data = []
            if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    try:
                        content = await f.read()
                        if content:
                            existing_data = json.loads(content)
                        if not isinstance(existing_data, list):
                            existing_data = [existing_data]
                    except json.JSONDecodeError:
                        existing_data = []
            existing_data.append(item)
            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(existing_data, ensure_ascii=False, indent=4))

# Logger setup
def init_logging():
    logger = logging.getLogger("MediaCrawler")
    logger.setLevel(logging.INFO)
    handler = RichHandler(rich_tracebacks=True, show_path=False)
    handler.setFormatter(logging.Formatter("%(asctime)s - %(message)s"))
    logger.addHandler(handler)
    return logger

logger = init_logging()

# -----------------------------------------------------------------------------
# Placeholder for merged code
# -----------------------------------------------------------------------------

async def main():
    """
    Main function to run the XiaoHongShu crawler.
    """
    # The check for a placeholder cookie is no longer needed as we are loading from a file.
    # if config.COOKIE == "YOUR_COOKIE_HERE":
    #     logger.error("Please update the COOKIE value in the Config class before running.")
    #     return

    crawler = XiaoHongShuCrawler(config.CRAWLER_TYPE, config)
    await crawler.start()

if __name__ == '__main__':
    asyncio.run(main())

